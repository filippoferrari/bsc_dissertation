@article{Davison2008,
abstract = {Computational neuroscience has produced a diversity of software for simulations of networks of spiking neurons, with both negative and positive consequences. On the one hand, each simulator uses its own programming or configuration language, leading to considerable difficulty in porting models from one simulator to another. This impedes communication between investigators and makes it harder to reproduce and build on the work of others. On the other hand, simulation results can be cross-checked between different simulators, giving greater confidence in their correctness, and each simulator has different optimizations, so the most appropriate simulator can be chosen for a given modelling task. A common programming interface to multiple simulators would reduce or eliminate the problems of simulator diversity while retaining the benefits. PyNN is such an interface, making it possible to write a simulation script once, using the Python programming language, and run it without modification on any supported simulator (currently NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware). PyNN increases the productivity of neuronal network modelling by providing high-level abstraction, by promoting code sharing and reuse, and by providing a foundation for simulator-agnostic analysis, visualization and data-management tools. PyNN increases the reliability of modelling studies by making it much easier to check results on multiple simulators. PyNN is open-source software and is available from http://neuralensemble.org/PyNN.},
author = {Davison, Andrew P},
doi = {10.3389/neuro.11.011.2008},
file = {:Users/ff/Dropbox/Mendeley/Davison/PyNN a common interface for neuronal network simulators/Davison{\_}2008{\_}PyNN a common interface for neuronal network simulators.pdf:pdf},
isbn = {1662-5196 (Electronic)$\backslash$r1662-5196 (Linking)},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {computational,interoperability,large-scale models,neuroscience,parallel computing,python,reproducibility,simulation,translation},
mendeley-groups = {Third Year Project},
number = {January},
pages = {1--10},
pmid = {19194529},
title = {{PyNN: a common interface for neuronal network simulators}},
url = {http://journal.frontiersin.org/article/10.3389/neuro.11.011.2008/abstract},
volume = {2},
year = {2008}
}
@article{Folowosele2011,
abstract = {Object recognition and categorization are computationally difficult tasks that are performed effortlessly by humans. Attempts have been made to emulate the computations in different parts of the primate cortex to gain a better understanding of the cortex and to design brain–machine interfaces that speak the same language as the brain. The HMAX model proposed by Riesenhuber and Poggio and extended by Serre attempts to truly model the visual cortex. In this paper, we provide a spike-based implementation of the HMAX model, demonstrating its ability to perform biologically-plausible MAX computations as well as classify basic shapes. The spike-based model consists of 2514 neurons and 17{\$}thinspace{\$} 305 synapses (S1 Layer: 576 neurons and 7488 synapses, C1 Layer: 720 neurons and 2880 synapses, S2 Layer: 576 neurons and 1152 synapses, C2 Layer: 640 neurons and 5760 synapses, and Classifier: 2 neurons and 25 synapses). Without the limits of the retina model, it will take the system 2 min to recognize rectangles and triangles in 24{\$},times,{\$} 24 pixel images. This can be reduced to 4.8 s by rearranging the lookup table so that neurons which have similar responses to the same input(s) can be placed on the same row and affected in parallel.},
author = {Folowosele, Fopefolu and Vogelstein, R. Jacob and Etienne-Cummings, Ralph},
doi = {10.1109/JETCAS.2012.2183409},
file = {:Users/ff/Dropbox/Mendeley/Folowosele, Vogelstein, Etienne-Cummings/Towards a cortical prosthesis Implementing a spike-based HMAX model of visual object recognition in silico/Folowosele, Vogelstein, Etienne-Cummings{\_}2011{\_}Towards a cortical prosthesis Implementing a spike-based HMAX model of visual object recog.pdf:pdf},
isbn = {2156-3357 VO  - 1},
issn = {21563357},
journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
keywords = {Computer vision,neuromorphic vision,object recognition,spiking neurons},
mendeley-groups = {Third Year Project},
number = {4},
pages = {516--525},
title = {{Towards a cortical prosthesis: Implementing a spike-based HMAX model of visual object recognition in silico}},
volume = {1},
year = {2011}
}
@article{Furber2016,
abstract = {Neuromorphic computing covers a diverse range of approaches to information processing all of which demonstrate some degree of neurobiological inspiration that differentiates them from mainstream conventional computing systems. The philosophy behind neuromorphic computing has its origins in the seminal work carried out by Carver Mead at Caltech in the late 1980s. This early work influenced others to carry developments forward, and advances in VLSI technology supported steady growth in the scale and capability of neuromorphic devices. Recently, a number of large-scale neuromorphic projects have emerged, taking the approach to unprecedented scales and capabilities. These large-scale projects are associated with major new funding initiatives for brain-related research, creating a sense that the time and circumstances are right for progress in our understanding of information processing in the brain. In this review we present a brief history of neuromorphic engineering then focus on some of the principal current large-scale projects, their main features, how their approaches are complementary and distinct, their advantages and drawbacks, and highlight the sorts of capabilities that each can deliver to neural modellers.},
author = {Furber, Steve},
doi = {10.1088/1741-2560/13/5/051001},
file = {:Users/ff/Dropbox/Mendeley/Furber/Large-scale neuromorphic computing systems/Furber{\_}2016{\_}Large-scale neuromorphic computing systems.pdf:pdf},
keywords = {brain-inspired computing,large-scale neuromorphics,neuromorphic systems},
mendeley-groups = {Third Year Project},
title = {{Large-scale neuromorphic computing systems}},
url = {http://iopscience.iop.org/1741-2552/13/5/051001},
year = {2016}
}
@article{Furber,
abstract = {—SpiNNaker (a contraction of Spiking Neural Network Architecture) is a million-core computing engine whose flagship goal is to be able to simulate the behaviour of aggregates of up to a billion neurons in real time. It consists of an array of ARM9 cores, communicating via packets carried by a custom interconnect fabric. The packets are small (40 or 72 bits), and their transmission is brokered entirely by hardware, giving the overall engine an extremely high bisection bandwidth of over 5 billion packets/s. Three of the principle axioms of parallel machine design – memory coherence, synchronicity and determinism – have been discarded in the design without, surprisingly, compromising the ability to perform meaningful computations. A further attribute of the system is the acknowledgment, from the initial design stages, that the sheer size of the implementation will make component failures an inevitable aspect of day-to-day operation, and fault detection and recovery mechanisms have been built into the system at many levels of abstraction. This paper describes the architecture of the machine and outlines the underlying design philosophy; software and applications are to be described in detail elsewhere, and only introduced in passing here as necessary to illuminate the description.},
author = {Furber, Steve B and Lester, David R and Plana, Luis A and Garside, Jim D and Painkras, Eustace and Temple, Steve and Brown, Andrew D},
file = {:Users/ff/Dropbox/Mendeley/Furber et al/Overview of the SpiNNaker system architecture/Furber et al.{\_}2013{\_}Overview of the SpiNNaker system architecture.pdf:pdf},
journal = {IEEE Transactions on Computers},
keywords = {Index Terms— Interconnection architectures,neurocomputers,parallel processors,real-time distributed ——————————  ——————————},
mendeley-groups = {Third Year Project},
number = {12},
title = {{Overview of the SpiNNaker system architecture}},
url = {https://eprints.soton.ac.uk/350495/1/TCv2.pdf},
volume = {62},
year = {2013}
}
@article{Galluppi2012,
abstract = {Computation with spiking neurons takes advantage of the abstraction of action potentials into streams of stereotypical events, which encode information through their timing. This approach both reduces power consumption and alleviates communication bottlenecks. A num-ber of such spiking custom mixed-signal address event representation (AER) chips have been developed in recent years. In this paper, we present i) a flexible event-driven platform consisting of the integration of a visual AER sensor and the SpiNNaker system, a programmable massively parallel digital architecture oriented to the simulation of spiking neural networks; ii) the implementation of a neural network for feature-based attentional selection on this platform.},
author = {Galluppi, Francesco and Brohan, Kevin and Davidson, Simon},
file = {:Users/ff/Dropbox/Mendeley/Galluppi, Brohan, Davidson/A Real-Time, Event Driven Neuromorphic System for Goal-Directed Attentional Selection/Galluppi, Brohan, Davidson{\_}2012{\_}A Real-Time, Event Driven Neuromorphic System for Goal-Directed Attentional Selection.pdf:pdf},
keywords = {AER,Attention,Neuromorphic,Selection,SpiNNaker},
mendeley-groups = {Third Year Project},
title = {{A Real-Time, Event Driven Neuromorphic System for Goal-Directed Attentional Selection}},
url = {https://pdfs.semanticscholar.org/9ffa/11f4aaf3211da1a2fdd9c787bcb957deaa16.pdf},
year = {2012}
}
@inproceedings{Garcia2017,
abstract = {Vision is one of our most important senses, a vast amount of information is perceived through our eyes. Neuroscientists have performed many studies using vision as input to their experiments. Computational neuroscientists have typically used a brightness-to-rate encoding to use images as spike-based visual sources for its natural mapping. Recently, neuromorphic Dynamic Vision Sensors (DVSs) were developed and, while they have excellent capabilities, they remain scarce and relatively expensive. We propose a visual input system inspired by the behaviour of a DVS but using a conventional digital camera as a sensor and a PC to encode the images. By using readily-available components, we believe most scientists would have access to a realistic spiking visual input source. While our primary goal is to provide systems with a live real-time input, we have also been successful in transcoding well established image and video databases into spike train representations. Our main contribution is a DVS emulator framework which can be extended, as we demonstrate by adding local inhibitory behaviour, adaptive thresholds and spike-timing encoding. {\textcopyright} 2016 IEEE.},
author = {Garcia, Garibaldi Pineda and Camilleri, Patrick and Liu, Qian and Furber, Steve},
booktitle = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
doi = {10.1109/SSCI.2016.7850249},
file = {:Users/ff/Dropbox/Mendeley/Garcia et al/PyDVS An extensible, real-time Dynamic Vision Sensor emulator using off-the-shelf hardware/Garcia et al.{\_}2017{\_}PyDVS An extensible, real-time Dynamic Vision Sensor emulator using off-the-shelf hardware.pdf:pdf},
isbn = {9781509042401},
mendeley-groups = {Third Year Project},
month = {dec},
pages = {1--7},
publisher = {IEEE},
title = {{PyDVS: An extensible, real-time Dynamic Vision Sensor emulator using off-the-shelf hardware}},
url = {http://ieeexplore.ieee.org/document/7850249/},
year = {2017}
}
@inproceedings{Glover2016,
abstract = {The fast temporal-dynamics and intrinsic motion segmentation of event-based cameras are beneficial for robotic tasks that require low-latency visual tracking and control, for example a robot catching a ball. When the event-driven iCub humanoid robot grasps an object its head and torso move, inducing camera motion, and tracked objects become no longer trivially segmented amongst the mass of background clutter. Current event-based tracking algorithms have mostly considered stationary cameras that have clean event-streams with minimal clutter. This paper introduces novel methods to extend the Hough-based circle detection algorithm using optical flow information that is readily extracted from the spatio-temporal event space. Results indicate the proposed directed-Hough algorithm is more robust to other moving objects and the background event-clutter. Finally, we demonstrate successful on-line robot control and gaze following on the iCub robot.},
author = {Glover, Affen and Bartolozzi, Chiara},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759345},
file = {:Users/ff/Dropbox/Mendeley/Glover, Bartolozzi/Event-driven ball detection and gaze fixation in elntter/Glover, Bartolozzi{\_}2016{\_}Event-driven ball detection and gaze fixation in elntter.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
mendeley-groups = {Third Year Project},
pages = {2203--2208},
title = {{Event-driven ball detection and gaze fixation in elntter}},
volume = {2016-Novem},
year = {2016}
}
@article{Glover2017,
author = {Glover, Arren and Bartolozzi, Chiara},
doi = {10.1109/IROS.2017.8206226},
file = {:Users/ff/Dropbox/Mendeley/Glover, Bartolozzi/Robust visual tracking with a freely-moving event camera/Glover, Bartolozzi{\_}2017{\_}Robust visual tracking with a freely-moving event camera.pdf:pdf;:Users/ff/Dropbox/Mendeley/Glover, Bartolozzi/Robust visual tracking with a freely-moving event camera/Glover, Bartolozzi{\_}2017{\_}Robust visual tracking with a freely-moving event camera(2).pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Computer Vision for Other Robotic,Visual Tracking},
mendeley-groups = {Third Year Project},
pages = {3769--3776},
title = {{Robust visual tracking with a freely-moving event camera}},
volume = {2017-Septe},
year = {2017}
}
@article{HernandezGarcia2018,
abstract = {Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred' orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors.},
author = {{Hern{\'{a}}ndez Garc{\'{i}}a}, Daniel and Adams, Samantha and Rast, Alex and Wennekers, Thomas and Furber, Steve and Cangelosi, Angelo},
doi = {10.1016/j.robot.2018.02.010},
file = {:Users/ff/Dropbox/Mendeley/Hern{\'{a}}ndez Garc{\'{i}}a et al/Visual attention and object naming in humanoid robots using a bio-inspired spiking neural network/Hern{\'{a}}ndez Garc{\'{i}}a et al.{\_}2018{\_}Visual attention and object naming in humanoid robots using a bio-inspired spiking neural network.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Biological inspired models,Neurorobotics,Object naming,Spiking neural networks,Visual attention},
mendeley-groups = {Third Year Project},
pages = {56--71},
title = {{Visual attention and object naming in humanoid robots using a bio-inspired spiking neural network}},
volume = {104},
year = {2018}
}
@article{Hopkins2018,
abstract = {State-of-the-art computer vision systems use frame-based cameras that sample the visual scene as a series of high-resolution images. These are then processed using convolutional neural networks using neurons with continuous outputs. Biological vision systems use a quite different approach, where the eyes (cameras) sample the visual scene continuously, often with a non-uniformres- olution, and generate neural spike events in response to changes in the scene. The resulting spatio-temporal patterns of events are then processed through networks of spiking neurons. Such event-based processing offers advantages in terms of focusing constrained resources on the most salient features of the perceived scene, and those advantages should also accrue to engineered vision systems based upon similar principles. Event-based vision sensors, and event-based processing exemplified by the SpiNNaker (Spiking Neural Network Architecture) machine, can be used to model the biological vision pathway at various levels of detail. Herewe use this approach to explore struc- tural synaptic plasticity as a possible mechanism whereby biological vision systems may learn the statistics of their inputs without supervision, pointing theway to engineered vision systems with similar online learning capabilities. 1.},
author = {Hopkins, Michael and Pineda-Garc{\'{i}}a, Garibaldi and Bogdan, Petruț Antoniu P.A. and Furber, Steve B. S.B. and Pineda-garcı, Garibaldi and Bogdan, Petruț Antoniu P.A. and Furber, Steve B. S.B. and Pineda-Garcia, G. and Bogdan, Petruț Antoniu P.A. and Furber, Steve B. S.B.},
doi = {10.1098/rsfs.2018.0007},
file = {:Users/ff/Dropbox/Mendeley/Hopkins et al/Spiking neural networks for computer vision/Hopkins et al.{\_}2018{\_}Spiking neural networks for computer vision.pdf:pdf},
issn = {2042-8898},
journal = {Royal Society Interface Focus},
keywords = {Computer vision,Neuromorphic computing,SpiNNaker,Spiking neural networks,Structural plasticity,Subject Areas,computational biology,computational biology Keywords,computer vision,neuromorphic computing,spiking neural networks,structural plasticity},
mendeley-groups = {Third Year Project},
number = {4},
pmid = {29951187},
title = {{Spiking neural networks for computer vision}},
url = {http://rsfs.royalsocietypublishing.org/content/8/4/20180007},
volume = {8},
year = {2018}
}
@article{Kheradpisheh2018,
abstract = {Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated – using rate-based neural networks trained with back-propagation – that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We also demonstrate the superiority of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken together, our results suggest that the combination of STDP with latency coding may be a key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions.},
archivePrefix = {arXiv},
arxivId = {1611.01421},
author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J and Masquelier, Timoth{\'{e}}e},
doi = {10.1016/j.neunet.2017.12.005},
eprint = {1611.01421},
file = {:Users/ff/Dropbox/Mendeley/Kheradpisheh et al/STDP-based spiking deep convolutional neural networks for object recognition/Kheradpisheh et al.{\_}2018{\_}STDP-based spiking deep convolutional neural networks for object recognition.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Deep learning,Object recognition,STDP,Spiking neural network,Temporal coding},
mendeley-groups = {Third Year Project},
pages = {56--67},
pmid = {29328958},
title = {{STDP-based spiking deep convolutional neural networks for object recognition}},
url = {https://ac.els-cdn.com/S0893608017302903/1-s2.0-S0893608017302903-main.pdf?{\_}tid=310ba508-a40e-497a-b4dc-0c8f92d24ab3{\&}acdnat=1523194989{\_}38f291cb39b447a20d7fb0a6fa0e8a3b http://linkinghub.elsevier.com/retrieve/pii/S0893608017302903},
volume = {99},
year = {2018}
}
@incollection{Paugam-Moisy2012,
abstract = {Spiking Neuron Networks (SNNs) are often referred to as the 3 rd gener- ation of neural networks. Highly inspired from natural computing in the brain and recent advances in neurosciences, they derive their strength and interest from an ac- curate modeling of synaptic interactions between neurons, taking into account the time of spike firing. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation. Today, the main challenge is to discover efficient learning rules that might take advantage of the specific features of SNNs while keeping the nice properties (general-purpose, easy-to-use, available simulators, etc.) of traditional connectionist models. This chapter relates the his- tory of the “spiking neuron” in Section 1 and summarizes the most currently-in-use models of neurons and synaptic plasticity in Section 2. The computational power of SNNs is addressed in Section 3 and the problem of learning in networks of spiking neurons is tackled in Section 4, with insights into the tracks currently explored for solving it. Finally, Section 5 discusses application domains, implementation issues and proposes several simulation frameworks},
author = {Paugam-Moisy, H{\'{e}}l{\`{e}}ne and Bohte, Sander},
booktitle = {Handbook of Natural Computing},
doi = {10.1007/978-3-540-92910-9_10},
file = {:Users/ff/Dropbox/Mendeley/Paugam-Moisy, Bohte/Computing with spiking neuron networks/Paugam-Moisy, Bohte{\_}2012{\_}Computing with spiking neuron networks.pdf:pdf},
isbn = {9783540929109},
issn = {20748523},
mendeley-groups = {Third Year Project},
pages = {335--376},
pmid = {1000185226},
title = {{Computing with spiking neuron networks}},
url = {https://pdfs.semanticscholar.org/8997/95d8e3c07976f62149eb79612e06231d6aee.pdf},
volume = {1-4},
year = {2012}
}
@article{Riesenhuber1999,
abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
author = {Riesenhuber, Maximilian and Poggio, Tomaso},
doi = {10.1038/14819},
file = {:Users/ff/Dropbox/Mendeley/Riesenhuber, Poggio/Hierarchical models of object recognition in cortex/Riesenhuber, Poggio{\_}1999{\_}Hierarchical models of object recognition in cortex.pdf:pdf},
isbn = {1097-6256},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Animals,Computer Simulation,Form Perception,Form Perception: physiology,Macaca,Mental Recall,Mental Recall: physiology,Models,Neurological,Neurons,Neurons: physiology,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Fields,Visual Fields: physiology},
mendeley-groups = {Third Year Project},
month = {nov},
number = {11},
pages = {1019--25},
pmid = {10526343},
publisher = {Nature Publishing Group},
title = {{Hierarchical models of object recognition in cortex.}},
url = {http://www.nature.com/articles/nn1199{\_}1019 http://www.ncbi.nlm.nih.gov/pubmed/10526343},
volume = {2},
year = {1999}
}
